{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b080c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0aee0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('name.txt', 'r').read().splitlines()  # read names from file\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16f0eae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c78bb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of the characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))  # unique characters\n",
    "stoi = { ch: i+1 for i, ch in enumerate(chars) }  # character to integer mapping\n",
    "stoi['.'] = 0  # add a special character for padding\n",
    "itos = { i: ch for ch, i in stoi.items() }  # integer to character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3ed6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: emma\n",
      "context: ..., target: e\n",
      "context: ..e, target: m\n",
      "context: .em, target: m\n",
      "context: emm, target: a\n",
      "context: mma, target: .\n",
      "word: olivia\n",
      "context: ..., target: o\n",
      "context: ..o, target: l\n",
      "context: .ol, target: i\n",
      "context: oli, target: v\n",
      "context: liv, target: i\n",
      "context: ivi, target: a\n",
      "context: via, target: .\n",
      "word: ava\n",
      "context: ..., target: a\n",
      "context: ..a, target: v\n",
      "context: .av, target: a\n",
      "context: ava, target: .\n",
      "word: isabella\n",
      "context: ..., target: i\n",
      "context: ..i, target: s\n",
      "context: .is, target: a\n",
      "context: isa, target: b\n",
      "context: sab, target: e\n",
      "context: abe, target: l\n",
      "context: bel, target: l\n",
      "context: ell, target: a\n",
      "context: lla, target: .\n",
      "word: sophia\n",
      "context: ..., target: s\n",
      "context: ..s, target: o\n",
      "context: .so, target: p\n",
      "context: sop, target: h\n",
      "context: oph, target: i\n",
      "context: phi, target: a\n",
      "context: hia, target: .\n"
     ]
    }
   ],
   "source": [
    "# generate dataset \n",
    "\n",
    "block_size = 3  # context length: how many characters to look at to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(f'word: {w}')\n",
    "    context = [0] * block_size  # start with a context of zeros (padding)\n",
    "    for ch in w + \".\":  # add a special character at the end\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)  # append the context to the input\n",
    "        Y.append(ix)  # append the target character\n",
    "        print(f'context: {''.join(itos[i] for i in context)}, target: {itos[ix]}')\n",
    "        context = context[1:] + [ix]  # update the context by removing the first character and adding the new one\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647) # set random seed\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # interested in the gradients of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "198892ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2043dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the lookup table\n",
    "# We have 27 possible characters (26 letters + 1 for padding), and want to crammed them into a 2 dimensional space\n",
    "# So we create a 27x2 matrix where each row corresponds to a character's embedding\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "393e2669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d5cc06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6098, 0.1214])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] # equivalent to F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad34f82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c16a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0818, -1.0367])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[13, 2] # equivalent to using the X[13, 2] as an index to get the embedding vector\n",
    "X[13, 2] # = 1\n",
    "C[1] # = emb[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the hidden layer\n",
    "# the input parameters number of every neural at the hidden layer is 6 (3 context characters * 2 embedding dimensions), and we construct a hidden layer with 100 neurons\n",
    "W1 = torch.randn(6, 100)\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65f2385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want emb @ W1 + b1, but we can't do that directly because emb is of shape (N, 3, 2) and W1 is of shape (6, 100)\n",
    "# so we need to reshape emb to (N, 6\n",
    "\n",
    "# remove a dimension by unbinding the second dimension (3 context characters) and then concatenate them\n",
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8f959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another efficient way to do this is to use view, it doesn't allocate new memory, just changes the shape\n",
    "# -1 means \"infer the size of this dimension based on the other dimensions\"\n",
    "emb.view(emb.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e1ef360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1)  # this is the hidden layer output\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a6f3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the final layer\n",
    "# the hidden layer output is of shape (N, 100), and we want to map it to the output layer of shape (N, 27), because we have 27 possible characters\n",
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d47d506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4754e-06, 1.9698e-06, 1.0230e-08, 5.0876e-07, 1.9002e-06, 3.6529e-04,\n",
       "         1.9373e-02, 9.0992e-06, 4.1839e-05, 3.5549e-04, 1.8947e-05, 4.1066e-02,\n",
       "         7.3381e-01, 5.6573e-06, 2.6627e-08, 2.0744e-06, 6.9948e-04, 9.5731e-02,\n",
       "         3.0842e-03, 8.8265e-02, 2.6530e-10, 1.0996e-09, 3.3233e-07, 3.0356e-05,\n",
       "         1.5450e-02, 4.4870e-07, 1.6862e-03],\n",
       "        [1.3305e-11, 2.5496e-10, 2.4087e-11, 1.0992e-11, 2.3981e-06, 4.3513e-09,\n",
       "         2.0229e-09, 2.8756e-06, 2.1448e-08, 7.2838e-06, 5.7898e-07, 5.8718e-02,\n",
       "         1.0990e-02, 4.1920e-07, 2.7499e-06, 8.4436e-09, 3.5361e-07, 9.2851e-01,\n",
       "         2.5382e-10, 2.3448e-07, 3.2023e-10, 9.6878e-10, 4.3005e-07, 1.6597e-08,\n",
       "         3.0451e-04, 1.3715e-03, 9.2999e-05],\n",
       "        [2.9805e-03, 1.4171e-09, 1.7995e-04, 1.5852e-09, 2.2596e-03, 1.0163e-04,\n",
       "         9.8196e-01, 1.3174e-05, 1.4989e-03, 5.7815e-03, 5.1857e-04, 4.4604e-04,\n",
       "         1.3809e-03, 9.7113e-04, 4.0225e-11, 4.4271e-06, 9.5547e-04, 5.7182e-04,\n",
       "         3.6295e-04, 2.5031e-06, 1.9040e-08, 1.9348e-12, 1.5461e-09, 2.1449e-06,\n",
       "         5.5270e-06, 2.7046e-10, 1.1221e-06],\n",
       "        [1.3878e-08, 7.7119e-05, 6.3643e-13, 9.5300e-06, 1.1557e-07, 6.0650e-03,\n",
       "         9.1754e-03, 1.9273e-14, 1.2610e-09, 2.2308e-08, 3.1825e-15, 1.8187e-10,\n",
       "         1.6118e-06, 6.8204e-09, 1.3698e-10, 6.8456e-09, 3.8586e-11, 3.4333e-08,\n",
       "         5.9856e-07, 9.8458e-01, 4.2757e-13, 2.6137e-11, 5.0734e-08, 7.2227e-09,\n",
       "         9.4626e-05, 2.5369e-10, 3.6525e-10],\n",
       "        [1.2290e-06, 1.6368e-12, 9.1600e-11, 8.2044e-14, 1.2520e-11, 1.2980e-11,\n",
       "         4.6567e-07, 3.4125e-11, 9.0602e-12, 3.7503e-09, 7.7026e-12, 2.0650e-07,\n",
       "         1.2695e-06, 1.7692e-12, 5.0385e-07, 6.0396e-06, 9.6073e-06, 9.9791e-01,\n",
       "         2.1911e-08, 1.8790e-03, 1.3008e-09, 2.8201e-07, 5.0598e-09, 2.4615e-10,\n",
       "         4.1697e-07, 2.2123e-06, 1.8843e-04],\n",
       "        [6.4754e-06, 1.9698e-06, 1.0230e-08, 5.0876e-07, 1.9002e-06, 3.6529e-04,\n",
       "         1.9373e-02, 9.0992e-06, 4.1839e-05, 3.5549e-04, 1.8947e-05, 4.1066e-02,\n",
       "         7.3381e-01, 5.6573e-06, 2.6627e-08, 2.0744e-06, 6.9948e-04, 9.5731e-02,\n",
       "         3.0842e-03, 8.8265e-02, 2.6530e-10, 1.0996e-09, 3.3233e-07, 3.0356e-05,\n",
       "         1.5450e-02, 4.4870e-07, 1.6862e-03],\n",
       "        [4.9925e-11, 1.2034e-09, 6.8072e-11, 1.5481e-10, 2.7785e-06, 2.8238e-08,\n",
       "         3.3179e-08, 9.4700e-06, 6.1322e-08, 1.0609e-05, 1.9993e-06, 1.2866e-01,\n",
       "         1.8975e-02, 7.3732e-07, 1.6315e-06, 1.3044e-08, 1.6249e-06, 8.4990e-01,\n",
       "         5.3656e-09, 1.9615e-06, 4.6101e-10, 8.3461e-10, 7.8793e-07, 5.3835e-08,\n",
       "         1.5049e-03, 7.1873e-04, 2.1438e-04],\n",
       "        [1.2846e-05, 1.1151e-09, 3.8426e-08, 3.3462e-10, 3.3121e-07, 2.1130e-06,\n",
       "         9.9740e-01, 1.3470e-11, 8.2039e-11, 3.2562e-09, 3.5977e-08, 2.1941e-12,\n",
       "         8.7614e-11, 4.0849e-08, 7.1231e-13, 1.1377e-05, 5.7667e-09, 2.0949e-07,\n",
       "         2.5273e-03, 4.1999e-05, 9.5448e-12, 5.6511e-16, 2.4825e-11, 2.3645e-08,\n",
       "         1.3429e-09, 6.8389e-13, 7.4234e-12],\n",
       "        [1.4773e-06, 5.8160e-06, 3.0738e-15, 1.2718e-05, 1.4138e-05, 3.0315e-07,\n",
       "         6.9851e-09, 1.3434e-09, 6.4824e-11, 1.8547e-05, 2.2804e-15, 2.1340e-09,\n",
       "         2.6700e-08, 2.4653e-13, 1.8128e-02, 1.6087e-09, 4.0623e-05, 7.5632e-02,\n",
       "         8.9765e-11, 6.8215e-03, 6.3389e-08, 4.3801e-08, 2.8684e-07, 2.9230e-11,\n",
       "         9.3258e-06, 8.9874e-01, 5.7675e-04],\n",
       "        [1.9461e-07, 2.1050e-12, 5.1910e-06, 8.5535e-13, 2.2752e-07, 1.7607e-11,\n",
       "         7.8006e-11, 4.4430e-06, 3.7834e-02, 2.1897e-06, 1.0360e-05, 9.6210e-01,\n",
       "         4.4435e-07, 1.5566e-05, 2.4978e-09, 9.0744e-09, 1.9803e-05, 5.2849e-06,\n",
       "         9.8099e-09, 2.9780e-10, 1.0401e-15, 1.0961e-09, 7.7451e-07, 3.0535e-08,\n",
       "         4.1250e-10, 1.7135e-06, 3.1245e-06],\n",
       "        [1.2225e-13, 2.8094e-05, 1.1299e-01, 1.2799e-06, 1.0364e-04, 7.7737e-06,\n",
       "         8.4595e-07, 2.8228e-06, 1.4665e-11, 2.3320e-05, 1.3027e-09, 4.1551e-02,\n",
       "         8.7323e-11, 5.5693e-03, 1.9096e-09, 2.4382e-05, 4.7344e-10, 7.5491e-02,\n",
       "         6.7561e-07, 1.1699e-06, 2.0707e-07, 1.1845e-03, 1.6537e-01, 1.6446e-09,\n",
       "         6.9369e-04, 5.9696e-01, 1.7539e-07],\n",
       "        [1.4466e-06, 2.7796e-08, 2.0999e-08, 1.5817e-07, 1.5104e-06, 1.7568e-07,\n",
       "         9.7724e-01, 1.1881e-04, 4.2797e-08, 1.9517e-06, 2.2649e-05, 1.1538e-12,\n",
       "         5.3517e-06, 2.5955e-12, 8.3961e-05, 3.9700e-08, 2.5632e-04, 7.2177e-03,\n",
       "         1.4792e-02, 1.0555e-05, 2.2095e-04, 7.1192e-07, 5.5208e-08, 8.6563e-07,\n",
       "         2.0042e-05, 3.1056e-11, 3.2364e-11],\n",
       "        [6.4754e-06, 1.9698e-06, 1.0230e-08, 5.0876e-07, 1.9002e-06, 3.6529e-04,\n",
       "         1.9373e-02, 9.0992e-06, 4.1839e-05, 3.5549e-04, 1.8947e-05, 4.1066e-02,\n",
       "         7.3381e-01, 5.6573e-06, 2.6627e-08, 2.0744e-06, 6.9948e-04, 9.5731e-02,\n",
       "         3.0842e-03, 8.8265e-02, 2.6530e-10, 1.0996e-09, 3.3233e-07, 3.0356e-05,\n",
       "         1.5450e-02, 4.4870e-07, 1.6862e-03],\n",
       "        [1.2611e-06, 1.4139e-10, 1.0913e-07, 1.0742e-11, 9.8365e-08, 9.9421e-07,\n",
       "         8.3617e-04, 4.7587e-07, 6.5327e-12, 3.6851e-07, 2.3190e-10, 2.9168e-08,\n",
       "         1.2982e-09, 8.6622e-11, 1.0575e-06, 4.3267e-03, 7.7657e-07, 9.6983e-01,\n",
       "         5.4517e-05, 2.4923e-02, 1.1672e-08, 3.2354e-08, 5.1490e-08, 4.6373e-08,\n",
       "         2.4047e-06, 2.4470e-05, 2.6731e-08],\n",
       "        [5.3068e-05, 1.4051e-08, 1.1036e-11, 1.1795e-04, 1.0264e-04, 3.0445e-06,\n",
       "         4.4284e-09, 1.0783e-06, 9.9453e-01, 7.0663e-09, 7.7712e-10, 4.6148e-04,\n",
       "         4.6971e-03, 9.3710e-12, 1.9940e-06, 2.1437e-08, 3.3644e-05, 6.3231e-08,\n",
       "         6.1634e-08, 8.4721e-08, 1.8439e-10, 6.8112e-12, 1.6743e-10, 2.6785e-09,\n",
       "         1.2640e-11, 1.8732e-08, 1.5569e-07],\n",
       "        [5.8583e-10, 2.3899e-08, 6.0625e-03, 1.1222e-07, 1.1033e-05, 2.2804e-07,\n",
       "         8.2007e-05, 3.5207e-08, 2.1154e-11, 1.5799e-08, 8.2639e-10, 8.8635e-03,\n",
       "         2.9920e-13, 2.4498e-03, 7.2825e-14, 2.7644e-03, 6.7128e-11, 3.5667e-07,\n",
       "         2.9378e-07, 1.4069e-06, 4.8309e-07, 7.9906e-06, 5.0508e-04, 2.6444e-10,\n",
       "         5.3618e-07, 9.7910e-01, 1.4628e-04],\n",
       "        [6.4754e-06, 1.9698e-06, 1.0230e-08, 5.0876e-07, 1.9002e-06, 3.6529e-04,\n",
       "         1.9373e-02, 9.0992e-06, 4.1839e-05, 3.5549e-04, 1.8947e-05, 4.1066e-02,\n",
       "         7.3381e-01, 5.6573e-06, 2.6627e-08, 2.0744e-06, 6.9948e-04, 9.5731e-02,\n",
       "         3.0842e-03, 8.8265e-02, 2.6530e-10, 1.0996e-09, 3.3233e-07, 3.0356e-05,\n",
       "         1.5450e-02, 4.4870e-07, 1.6862e-03],\n",
       "        [3.5648e-12, 2.6198e-13, 5.5844e-12, 3.2201e-16, 2.0541e-08, 1.4311e-12,\n",
       "         7.5663e-13, 5.8490e-09, 4.0473e-13, 2.8811e-07, 1.4916e-10, 3.7355e-07,\n",
       "         1.5306e-07, 1.3898e-11, 3.0782e-06, 2.4315e-10, 1.2883e-09, 9.9934e-01,\n",
       "         2.2468e-13, 1.8063e-08, 1.4026e-10, 3.5077e-10, 4.3911e-09, 1.1988e-11,\n",
       "         3.3009e-09, 6.5475e-04, 3.8739e-08],\n",
       "        [7.5607e-08, 4.8548e-09, 2.0856e-05, 8.8044e-10, 7.7178e-04, 4.6683e-06,\n",
       "         2.4604e-06, 5.6529e-03, 9.2323e-01, 1.8724e-05, 1.1959e-06, 6.9148e-02,\n",
       "         1.1993e-04, 7.1160e-06, 1.7390e-08, 1.7022e-06, 1.0172e-03, 7.6414e-07,\n",
       "         2.7854e-08, 4.7896e-10, 7.2594e-11, 5.1870e-12, 1.1803e-07, 3.2404e-08,\n",
       "         3.4436e-07, 2.0782e-09, 2.4650e-08],\n",
       "        [1.6753e-08, 2.7562e-10, 5.0114e-06, 1.8241e-07, 3.2829e-02, 1.3521e-05,\n",
       "         3.7896e-02, 2.8309e-07, 5.1622e-12, 9.0623e-07, 2.1814e-10, 1.1816e-07,\n",
       "         1.0319e-12, 1.9737e-05, 6.1766e-10, 1.3244e-04, 4.3125e-09, 9.2752e-01,\n",
       "         1.7103e-06, 3.8448e-06, 5.3581e-07, 4.4980e-10, 2.6316e-05, 2.6018e-12,\n",
       "         3.9858e-06, 1.5496e-03, 4.4796e-10],\n",
       "        [3.2540e-08, 1.9005e-08, 1.2036e-13, 1.4089e-07, 6.6384e-07, 3.0891e-07,\n",
       "         8.6004e-13, 3.4600e-09, 3.9252e-01, 2.8309e-08, 3.3085e-11, 1.7668e-05,\n",
       "         6.0745e-01, 3.9722e-13, 4.9947e-06, 9.1315e-10, 3.6133e-07, 7.6092e-09,\n",
       "         1.0184e-12, 2.8133e-08, 1.0974e-11, 1.3064e-11, 1.9227e-12, 8.7862e-09,\n",
       "         2.4214e-10, 3.3530e-08, 3.2858e-09],\n",
       "        [6.6822e-16, 4.7936e-09, 2.7756e-03, 2.8848e-10, 1.8327e-08, 1.2009e-06,\n",
       "         1.1901e-10, 3.8673e-11, 1.4614e-14, 1.9107e-05, 4.0166e-11, 1.2648e-02,\n",
       "         1.9017e-10, 7.9872e-03, 1.4427e-13, 1.3726e-10, 2.0259e-13, 1.1646e-05,\n",
       "         5.1626e-10, 6.2093e-10, 3.2969e-12, 4.8685e-03, 1.3746e-03, 4.3119e-11,\n",
       "         2.5468e-08, 9.6204e-01, 8.2704e-03],\n",
       "        [1.9746e-09, 1.2910e-07, 4.2057e-11, 1.9356e-08, 1.0820e-12, 9.4266e-08,\n",
       "         9.9998e-01, 4.5987e-12, 1.1342e-10, 4.8750e-14, 5.8876e-10, 5.4989e-19,\n",
       "         6.7725e-13, 1.0353e-10, 2.0032e-12, 7.7157e-09, 1.1731e-11, 7.7528e-12,\n",
       "         2.1233e-06, 1.5854e-05, 9.2835e-10, 6.7128e-12, 3.5661e-11, 3.9554e-09,\n",
       "         3.1879e-09, 4.8451e-20, 3.7956e-17],\n",
       "        [7.5546e-08, 5.2603e-06, 3.1936e-14, 1.8792e-05, 6.1199e-09, 3.0625e-08,\n",
       "         6.2021e-03, 7.7346e-15, 7.9293e-12, 1.1785e-07, 1.3783e-18, 2.9459e-14,\n",
       "         2.0306e-08, 6.4970e-13, 9.7338e-11, 3.1009e-09, 1.5757e-08, 3.1825e-09,\n",
       "         6.9168e-08, 9.9377e-01, 2.4745e-13, 9.0466e-10, 8.3923e-10, 2.3848e-09,\n",
       "         1.0768e-06, 6.0777e-09, 2.6116e-09],\n",
       "        [4.6235e-05, 1.0110e-11, 2.1623e-12, 3.6809e-13, 6.0988e-10, 7.3287e-14,\n",
       "         2.1555e-08, 2.1663e-11, 5.6559e-11, 8.1188e-09, 7.6097e-11, 3.2514e-06,\n",
       "         1.0305e-04, 6.0870e-12, 4.9469e-05, 4.1012e-07, 1.3814e-04, 6.9833e-01,\n",
       "         5.6398e-06, 2.7808e-01, 8.6029e-08, 1.3023e-06, 8.8775e-08, 1.0450e-08,\n",
       "         9.3832e-09, 3.8027e-05, 2.3210e-02],\n",
       "        [6.4754e-06, 1.9698e-06, 1.0230e-08, 5.0876e-07, 1.9002e-06, 3.6529e-04,\n",
       "         1.9373e-02, 9.0992e-06, 4.1839e-05, 3.5549e-04, 1.8947e-05, 4.1066e-02,\n",
       "         7.3381e-01, 5.6573e-06, 2.6627e-08, 2.0744e-06, 6.9948e-04, 9.5731e-02,\n",
       "         3.0842e-03, 8.8265e-02, 2.6530e-10, 1.0996e-09, 3.3233e-07, 3.0356e-05,\n",
       "         1.5450e-02, 4.4870e-07, 1.6862e-03],\n",
       "        [1.0200e-07, 5.4680e-07, 1.1132e-09, 1.2845e-06, 3.9277e-08, 2.8330e-04,\n",
       "         1.9703e-04, 4.8612e-08, 9.7023e-05, 2.6300e-05, 8.9146e-06, 7.1563e-01,\n",
       "         2.8366e-01, 1.4335e-05, 3.1817e-10, 6.4772e-10, 7.8512e-07, 1.7618e-06,\n",
       "         1.5927e-05, 2.7797e-05, 7.6140e-14, 8.1482e-11, 1.8168e-08, 1.8206e-08,\n",
       "         2.6784e-05, 8.8161e-12, 1.0437e-06],\n",
       "        [6.3098e-12, 7.2636e-10, 7.4967e-09, 1.0330e-11, 3.7474e-08, 1.6332e-04,\n",
       "         2.0337e-08, 4.5856e-06, 6.4136e-10, 7.1956e-05, 6.8744e-06, 8.4328e-01,\n",
       "         5.4534e-04, 7.1645e-03, 1.1922e-08, 2.4988e-08, 2.5657e-09, 1.8418e-04,\n",
       "         3.8887e-07, 4.8781e-06, 1.2372e-12, 1.5662e-08, 9.4276e-06, 3.5601e-08,\n",
       "         1.4848e-01, 6.8623e-05, 1.0063e-05],\n",
       "        [3.4607e-11, 7.6399e-08, 1.4371e-06, 5.7047e-07, 1.4225e-04, 5.0980e-02,\n",
       "         9.2258e-08, 1.0290e-02, 2.8712e-02, 9.7577e-04, 1.8931e-04, 2.4307e-02,\n",
       "         1.9836e-01, 1.1158e-06, 1.3355e-05, 2.6445e-08, 5.2734e-05, 2.7750e-06,\n",
       "         9.1109e-08, 2.2945e-08, 3.0547e-07, 1.7154e-10, 5.9418e-06, 2.7718e-08,\n",
       "         6.8596e-01, 4.4788e-09, 7.5037e-07],\n",
       "        [2.4252e-15, 3.1689e-07, 2.6960e-02, 3.6716e-11, 4.4557e-07, 1.1653e-06,\n",
       "         1.9082e-07, 2.2188e-06, 1.9892e-12, 1.0329e-08, 2.1981e-06, 2.0185e-05,\n",
       "         5.2067e-10, 9.5613e-01, 9.3787e-11, 4.9626e-08, 3.0456e-13, 4.8028e-06,\n",
       "         3.4292e-07, 3.4293e-09, 2.6682e-09, 6.5615e-08, 6.9096e-03, 1.2979e-08,\n",
       "         9.9597e-03, 1.2373e-05, 4.5038e-11],\n",
       "        [1.4112e-11, 5.2496e-06, 3.5456e-08, 2.9811e-05, 7.4879e-06, 5.8020e-08,\n",
       "         2.9763e-05, 2.6183e-02, 5.1760e-08, 5.2196e-03, 9.0510e-10, 4.8054e-10,\n",
       "         8.9479e-06, 2.4826e-15, 7.2784e-01, 3.3804e-10, 7.2051e-04, 3.1221e-02,\n",
       "         2.1140e-08, 2.1814e-05, 2.0224e-01, 4.0460e-06, 6.4695e-05, 8.8296e-09,\n",
       "         6.3989e-03, 1.6191e-07, 2.5182e-08],\n",
       "        [4.1378e-04, 4.1958e-13, 6.6228e-05, 8.4992e-10, 5.4331e-01, 7.4262e-08,\n",
       "         1.1030e-02, 3.1145e-02, 1.5351e-08, 5.8802e-04, 1.0436e-09, 2.5916e-07,\n",
       "         1.3736e-13, 5.8104e-07, 5.6611e-05, 1.4033e-05, 1.2347e-04, 4.0677e-01,\n",
       "         5.6684e-05, 2.3295e-07, 4.3089e-07, 3.4959e-07, 1.8227e-09, 7.5189e-09,\n",
       "         8.6613e-11, 6.4247e-03, 2.2615e-09]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2  # this is the output layer\n",
    "counts = logits.exp()  # convert logits to counts\n",
    "probs = counts / counts.sum(1, keepdim=True)  # convert counts to probabilities\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "885ecd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6529e-04, 4.1920e-07, 9.7113e-04, 7.7119e-05, 1.2290e-06, 2.0744e-06,\n",
       "        1.8975e-02, 3.2562e-09, 2.8684e-07, 2.1897e-06, 2.8094e-05, 1.4466e-06,\n",
       "        1.9698e-06, 5.1490e-08, 1.4051e-08, 5.8583e-10, 3.5549e-04, 1.8063e-08,\n",
       "        4.8548e-09, 5.0114e-06, 3.0891e-07, 1.9017e-10, 6.7725e-13, 5.2603e-06,\n",
       "        4.6235e-05, 8.8265e-02, 6.4772e-10, 2.5657e-09, 2.8712e-02, 1.0329e-08,\n",
       "        5.2496e-06, 4.1378e-04])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can use Y to get the correct probabilities for the target characters which generated fromt the network\n",
    "probs[torch.arange(len(Y)), Y]  # this is the probability of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69da14b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.7622)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(len(Y)), Y].log()  # negative log likelihood loss\n",
    "loss.mean()  # average loss over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd83f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- rewrite the above code ----- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "514f5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset \n",
    "\n",
    "block_size = 3  # context length: how many characters to look at to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size  # start with a context of zeros (padding)\n",
    "    for ch in w + \".\":  # add a special character at the end\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)  # append the context to the input\n",
    "        Y.append(ix)  # append the target character\n",
    "        context = context[1:] + [ix]  # update the context by removing the first character and adding the new one\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "Y = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5600751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c7fdb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)  # set random seed\n",
    "C = torch.randn((27, 2), generator=g)  # character embeddings\n",
    "W1 = torch.randn(6, 100, generator=g)  # hidden layer weights\n",
    "b1 = torch.randn(100, generator=g)  # hidden layer biases\n",
    "W2 = torch.randn(100, 27, generator=g)  # output layer weights\n",
    "b2 = torch.randn(27, generator=g)  # output layer biases\n",
    "parameters = [C, W1, b1, W2, b2]  # list of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2cbda0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in parameters)  # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7236b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True  # we want to compute gradients for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59d0ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3644161224365234\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):  # run for 10 iterations\n",
    "    # it's much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient\n",
    "    # and take fewer steps\n",
    "    ix = torch.randint(0, len(X), (32,), generator=g)  # random batch of indices\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]  # lookup table for the characters\n",
    "    h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1)  # hidden layer output\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    # counts = logits.exp()  # convert logits to counts\n",
    "    # probs = counts / counts.sum(1, keepdim=True)  # convert counts to probabilities\n",
    "    # loss = -probs[torch.arange(len(Y)), Y].log()  # negative log likelihood loss\n",
    "    # loss.mean()  # average loss over the batch\n",
    "    loss = F.cross_entropy(logits, Y[ix])  # this is the same as the above loss, but more efficient and numerically stable\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None  # zero the gradients\n",
    "    loss.backward()  # compute gradients\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad  # update parameters with a learning rate of 0.1\n",
    "print(loss.item())  # print the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a32eedc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6146, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global loss\n",
    "emb = C[X]  # lookup table for the characters\n",
    "h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1)  # hidden layer output\n",
    "logits = h @ W2 + b2  # output layer\n",
    "loss = F.cross_entropy(logits, Y)  # compute the loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc53914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, dev or validation split, test split\n",
    "# 80% training use to train models, 10% validation use to train hyperparameters like the size of hidden layer, or the size of embedding, 10% test use to evaluate the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe78173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956eb897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107fd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234f010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4ee37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce29b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defa196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c839df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35b350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
